{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to PyTorch !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch is a python framework for deep learning tasks. It was tailored to be fast and pythnonic(Yeah!). The biggest                    advantage is its ability to automatically calculate gradients for the specified variables.The autograd package provides automatic differentiation for all operations on variables.This is very importatnt in  case of deep learning, as calculating gradients during back-propogation becomes hassle free.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "#import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor \n",
    "\n",
    "it's a n-diamensional array which resides on the gpu(mostly of the cases).\n",
    "\n",
    "Types supported:\n",
    "\n",
    "    64-bit (Float + Int)\n",
    "    32-bit (Float + Int)\n",
    "    16-bit (Float + Int)\n",
    "    8-bit (Signed + Unsigned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating a 2x3 tensor.\n",
    "x = torch.Tensor(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a 2x3 tensor with values randomly selected from a Uniform Distribution between -1 and 1\n",
    "y = torch.Tensor(2, 3)\n",
    "y = y.uniform_(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.7637 -0.4427 -0.6505\n",
      "-0.2531  0.3945 -0.9705\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-0.8576 -0.6813 -0.5944\n",
       "-0.3233  0.9764  0.6180\n",
       "[torch.FloatTensor of size 2x3]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add and storing result in another tensor.\n",
    "x.uniform_(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-0.0940 -1.1240 -1.2449\n",
       "-0.5764  1.3709 -0.3525\n",
       "[torch.FloatTensor of size 2x3]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = torch.Tensor(1, 1)\n",
    "torch.add(x, y, out=result)\n",
    "# notice how result got broadcasted into an tensor of 2x3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1.5273 -0.8854 -1.3010\n",
      "-0.5062  0.7889 -1.9411\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# normal multiplication\n",
    "print(torch.mul(y, 2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using In-line functions.. not the '_', helps in faster execution time. Here adding y with itself and storing \n",
    "z = y.add(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 3.0546 -1.7708 -2.6019\n",
       "-1.0125  1.5779 -3.8822\n",
       "[torch.FloatTensor of size 2x3]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 3.0546 -1.7708 -2.6019\n",
       "-1.0125  1.5779 -3.8822\n",
       "[torch.FloatTensor of size 2x3]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.add_(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 3.0546 -1.7708 -2.6019\n",
       "-1.0125  1.5779 -3.8822\n",
       "[torch.FloatTensor of size 2x3]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-0.0940 -1.1240 -1.2449\n",
       "-0.5764  1.3709 -0.3525\n",
       "[torch.FloatTensor of size 2x3]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.0546288 -1.7707987 -2.601944 ]\n",
      " [-1.0124555  1.5778947 -3.882174 ]]\n"
     ]
    }
   ],
   "source": [
    "#Converting Tensors to Numpy arrays.\n",
    "nampy = y.numpy()\n",
    "print(nampy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nampy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 5\n",
      " 0\n",
      "[torch.FloatTensor of size 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#moving the whole operations to GPU.\n",
    "if torch.cuda.is_available():\n",
    "    u = (y + y).cuda()\n",
    "print(y) # notice (GPU0) at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 3.0546 -1.7708 -2.6019\n",
      "-1.0125  1.5779 -3.8822\n",
      "[torch.FloatTensor of size 2x3]\n",
      " \n",
      "1.00000e-26 *\n",
      "  7.0044\n",
      "[torch.FloatTensor of size 1x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#moving the whole operations to GPU.\n",
    "u = torch.Tensor(1,1)\n",
    "if torch.cuda.is_available():\n",
    "    y = y.cuda()\n",
    "    u = y+y\n",
    "    u = u.cuda()\n",
    "print(y, u) # notice (GPU0) at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable\n",
    " __autograd.Variable__ is the central class of the package. It wraps a Tensor, and supports nearly all of operations defined on it. Once you finish your computation you can call __.backward()__ and have all the gradients computed automatically.\n",
    "![alt text](http://pytorch.org/tutorials/_images/Variable.png \"Variable Structure\")\n",
    "\n",
    "You can access the raw tensor through the __.data__ attribute, while the gradient w.r.t. this variable is accumulated into __.grad__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import Variable from pytorch.\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating a dot product of two matrices.\n",
    "\n",
    "x = Variable(torch.FloatTensor([10, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 10\n",
       " 10\n",
       "[torch.FloatTensor of size 2]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = Variable(torch.FloatTensor([5, 0]), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 5\n",
       " 0\n",
       "[torch.FloatTensor of size 2]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 25\n",
       "  0\n",
       "[torch.FloatTensor of size 2]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z = x.dot(y*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 250\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value of I : \n",
      " 250\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "z.backward(retain_graph=True)# for computing gradients automatically.\n",
    "print(f'value of I : {z.data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = x.dot(y*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-ecf7d37033d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# for computing gradients automatically.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'value of I : {z.data}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "i.backward(retain_graph=False)# for computing gradients automatically.\n",
    "print(f'value of I : {z.data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-08\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m        \n",
       "\u001b[0;32mclass\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Implements Adam algorithm.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    It has been proposed in `Adam: A Method for Stochastic Optimization`_.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Arguments:\u001b[0m\n",
       "\u001b[0;34m        params (iterable): iterable of parameters to optimize or dicts defining\u001b[0m\n",
       "\u001b[0;34m            parameter groups\u001b[0m\n",
       "\u001b[0;34m        lr (float, optional): learning rate (default: 1e-3)\u001b[0m\n",
       "\u001b[0;34m        betas (Tuple[float, float], optional): coefficients used for computing\u001b[0m\n",
       "\u001b[0;34m            running averages of gradient and its square (default: (0.9, 0.999))\u001b[0m\n",
       "\u001b[0;34m        eps (float, optional): term added to the denominator to improve\u001b[0m\n",
       "\u001b[0;34m            numerical stability (default: 1e-8)\u001b[0m\n",
       "\u001b[0;34m        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    .. _Adam\\: A Method for Stochastic Optimization:\u001b[0m\n",
       "\u001b[0;34m        https://arxiv.org/abs/1412.6980\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                 \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mdefaults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbetas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                        \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Performs a single optimization step.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        Arguments:\u001b[0m\n",
       "\u001b[0;34m            closure (callable, optional): A closure that reevaluates the model\u001b[0m\n",
       "\u001b[0;34m                and returns the loss.\u001b[0m\n",
       "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mclosure\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                    \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;32mif\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                    \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Adam does not support sparse gradients, please consider SparseAdam instead'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;31m# State initialization\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                    \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                    \u001b[0;31m# Exponential moving average of gradient values\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                    \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'exp_avg'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                    \u001b[0;31m# Exponential moving average of squared gradient values\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                    \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'exp_avg_sq'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avg_sq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'exp_avg'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'exp_avg_sq'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'betas'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;32mif\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                    \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mbias_correction1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mbias_correction2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m           ~/anaconda3/envs/torch/lib/python3.6/site-packages/torch/optim/adam.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "??optim.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-9c5820e808d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "x.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 400\n",
       "   0\n",
       "[torch.FloatTensor of size 2]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-c150c6715d07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#runing backward pass for the second time.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "#runing backward pass for the second time.\n",
    "z.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value of I : \n",
      " 250\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'value of I : {z.data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0\n",
       " 0\n",
       "[torch.FloatTensor of size 2]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 500\n",
       "   0\n",
       "[torch.FloatTensor of size 2]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad.data\n",
    "#Here the resultant gradient is erroneous according to our actual input.\n",
    "#This is because while, calculating the gradiets during the second pass, they get added with the gradients from the first pass.\n",
    "#Initializing weights to zero after each pass, solves the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.FloatTensor([10, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Variable(torch.FloatTensor([5, 0]), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = x.dot(y*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients form the first run : \n",
      " 100\n",
      "   0\n",
      "[torch.FloatTensor of size 2]\n",
      "\n",
      "Gradients form the first run : \n",
      " 200\n",
      "   0\n",
      "[torch.FloatTensor of size 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "z.backward(retain_graph=True)\n",
    "print(f'Gradients form the first run : {y.grad.data}')\n",
    "\n",
    "#Uncomment the line below to understand the error.\n",
    "#y.grad.data.zero_() # weights --> 0\n",
    "\n",
    "z.backward()\n",
    "print(f'Gradients form the first run : {y.grad.data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0\n",
       " 0\n",
       "[torch.FloatTensor of size 2]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [1.0, 8.0, 27.0]\n",
    "\n",
    "w = Variable(torch.Tensor([2.0]),  requires_grad=True)  # Any random value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict (before training) 4 4.0\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tgrad:  1.0 1.0 0.0\n",
      "\tgrad:  2.0 4.0 -8.0\n",
      "\tgrad:  3.0 9.0 -34.55999755859375\n",
      "progress: 0 33.17759704589844\n",
      "\tgrad:  1.0 1.0 0.8512001037597656\n",
      "\tgrad:  2.0 4.0 -4.663295745849609\n",
      "\tgrad:  3.0 9.0 -27.65302276611328\n",
      "progress: 1 21.241378784179688\n",
      "\tgrad:  1.0 1.0 1.4805026054382324\n",
      "\tgrad:  2.0 4.0 -2.196430206298828\n",
      "\tgrad:  3.0 9.0 -22.546611785888672\n",
      "progress: 2 14.120824813842773\n",
      "\tgrad:  1.0 1.0 1.9457533359527588\n",
      "\tgrad:  2.0 4.0 -0.3726472854614258\n",
      "\tgrad:  3.0 9.0 -18.771381378173828\n",
      "progress: 3 9.787908554077148\n",
      "\tgrad:  1.0 1.0 2.2897186279296875\n",
      "\tgrad:  2.0 4.0 0.9756965637207031\n",
      "\tgrad:  3.0 9.0 -15.98030948638916\n",
      "progress: 4 7.093619346618652\n",
      "\tgrad:  1.0 1.0 2.5440163612365723\n",
      "\tgrad:  2.0 4.0 1.972543716430664\n",
      "\tgrad:  3.0 9.0 -13.916833877563477\n",
      "progress: 5 5.3799519538879395\n",
      "\tgrad:  1.0 1.0 2.7320218086242676\n",
      "\tgrad:  2.0 4.0 2.7095260620117188\n",
      "\tgrad:  3.0 9.0 -12.391282081604004\n",
      "progress: 6 4.26510763168335\n",
      "\tgrad:  1.0 1.0 2.871016502380371\n",
      "\tgrad:  2.0 4.0 3.254384994506836\n",
      "\tgrad:  3.0 9.0 -11.263423919677734\n",
      "progress: 7 3.524019956588745\n",
      "\tgrad:  1.0 1.0 2.9737768173217773\n",
      "\tgrad:  2.0 4.0 3.657205581665039\n",
      "\tgrad:  3.0 9.0 -10.429584503173828\n",
      "progress: 8 3.021562099456787\n",
      "\tgrad:  1.0 1.0 3.0497488975524902\n",
      "\tgrad:  2.0 4.0 3.955015182495117\n",
      "\tgrad:  3.0 9.0 -9.813117027282715\n",
      "progress: 9 2.67492413520813\n",
      "predict (after training) 4 10.211832046508789\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict (before training) 4 2.680000066757202\n",
      "\tgrad:  1.0 1.0 1.492537260055542\n",
      "\tgrad:  2.0 4.0 25.45169448852539\n",
      "\tgrad:  3.0 9.0 141.47311401367188\n",
      "\tgrad:  4.0 16.0 -27.641857147216797\n",
      "\tgrad:  5.0 25.0 -59.479637145996094\n",
      "\tgrad:  6.0 36.0 -364.861328125\n",
      "progress: 0 nan\n",
      "\tgrad:  1.0 1.0 0.2852534055709839\n",
      "\tgrad:  2.0 4.0 2.141009569168091\n",
      "\tgrad:  3.0 9.0 5.126420497894287\n",
      "\tgrad:  4.0 16.0 9.381340026855469\n",
      "\tgrad:  5.0 25.0 15.145589828491211\n",
      "\tgrad:  6.0 36.0 22.89982795715332\n",
      "progress: 1 nan\n",
      "\tgrad:  1.0 1.0 0.33831095695495605\n",
      "\tgrad:  2.0 4.0 2.578047275543213\n",
      "\tgrad:  3.0 9.0 6.159935474395752\n",
      "\tgrad:  4.0 16.0 11.320375442504883\n",
      "\tgrad:  5.0 25.0 18.489431381225586\n",
      "\tgrad:  6.0 36.0 28.6054630279541\n",
      "progress: 2 nan\n",
      "\tgrad:  1.0 1.0 0.4384147822856903\n",
      "\tgrad:  2.0 4.0 3.4456918239593506\n",
      "\tgrad:  3.0 9.0 8.205265998840332\n",
      "\tgrad:  4.0 16.0 15.26042366027832\n",
      "\tgrad:  5.0 25.0 25.732025146484375\n",
      "\tgrad:  6.0 36.0 42.673431396484375\n",
      "progress: 3 nan\n",
      "\tgrad:  1.0 1.0 0.7556337714195251\n",
      "\tgrad:  2.0 4.0 6.71710205078125\n",
      "\tgrad:  3.0 9.0 15.947696685791016\n",
      "\tgrad:  4.0 16.0 32.56426239013672\n",
      "\tgrad:  5.0 25.0 75.32949829101562\n",
      "\tgrad:  6.0 36.0 3288.238037109375\n",
      "progress: 4 -102.61351776123047\n",
      "\tgrad:  1.0 1.0 -0.030420904979109764\n",
      "\tgrad:  2.0 4.0 -0.21158093214035034\n",
      "\tgrad:  3.0 9.0 -0.514750063419342\n",
      "\tgrad:  4.0 16.0 -0.9398194551467896\n",
      "\tgrad:  5.0 25.0 -1.4869743585586548\n",
      "\tgrad:  6.0 36.0 -2.1565966606140137\n",
      "progress: 5 nan\n",
      "\tgrad:  1.0 1.0 -0.030470406636595726\n",
      "\tgrad:  2.0 4.0 -0.2119230329990387\n",
      "\tgrad:  3.0 9.0 -0.515583872795105\n",
      "\tgrad:  4.0 16.0 -0.9413439035415649\n",
      "\tgrad:  5.0 25.0 -1.4893895387649536\n",
      "\tgrad:  6.0 36.0 -2.1601040363311768\n",
      "progress: 6 nan\n",
      "\tgrad:  1.0 1.0 -0.030520152300596237\n",
      "\tgrad:  2.0 4.0 -0.21226681768894196\n",
      "\tgrad:  3.0 9.0 -0.5164216756820679\n",
      "\tgrad:  4.0 16.0 -0.9428757429122925\n",
      "\tgrad:  5.0 25.0 -1.4918161630630493\n",
      "\tgrad:  6.0 36.0 -2.163628578186035\n",
      "progress: 7 nan\n",
      "\tgrad:  1.0 1.0 -0.03057014010846615\n",
      "\tgrad:  2.0 4.0 -0.21261225640773773\n",
      "\tgrad:  3.0 9.0 -0.5172637104988098\n",
      "\tgrad:  4.0 16.0 -0.9444151520729065\n",
      "\tgrad:  5.0 25.0 -1.4942548274993896\n",
      "\tgrad:  6.0 36.0 -2.167170286178589\n",
      "progress: 8 nan\n",
      "\tgrad:  1.0 1.0 -0.03062037192285061\n",
      "\tgrad:  2.0 4.0 -0.2129593789577484\n",
      "\tgrad:  3.0 9.0 -0.5181096792221069\n",
      "\tgrad:  4.0 16.0 -0.9459619522094727\n",
      "\tgrad:  5.0 25.0 -1.4967050552368164\n",
      "\tgrad:  6.0 36.0 -2.170729160308838\n",
      "progress: 9 nan\n",
      "predict (after training) 4 -130.4169921875\n"
     ]
    }
   ],
   "source": [
    "x_data = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0]\n",
    "y_data = [1.0, 4.0, 9.0, 16.0, 25.0, 36.0]\n",
    "\n",
    "w = Variable(torch.Tensor([.67]),  requires_grad=True)  # - Change this\n",
    "# our model forward pass\n",
    "from torch.nn import functional as F\n",
    "def forward(x):\n",
    "    return (x * w)   # - Change this\n",
    "\n",
    "# Loss function - Change this\n",
    "def loss(x, y):\n",
    "    y_pred = forward(x)\n",
    "    return y*torch.log(y_pred) - (1-y)*torch.log(1-y_pred)  #(y_pred - y) * (y_pred - y)\n",
    "\n",
    "# Before training\n",
    "print(\"predict (before training)\",  4, forward(4).data[0])\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):  # - Change this\n",
    "    for x_val, y_val in zip(x_data, y_data):\n",
    "        l = loss(x_val, y_val)\n",
    "        l.backward()\n",
    "        print(\"\\tgrad: \", x_val, y_val, w.grad.data[0])\n",
    "        w.data = w.data - 0.01 * w.grad.data    # - Change this\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w.grad.data.zero_()\n",
    "\n",
    "    print(\"progress:\", epoch, l.data[0])\n",
    "\n",
    "# After training\n",
    "print(\"predict (after training)\", 4, forward(4).data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# our model forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward(x):\n",
    "    return x * w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loss function\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(x, y):\n",
    "    y_pred = forward(x)\n",
    "    return (y_pred - y) * (y_pred - y)\n",
    "\n",
    "# Before training\n",
    "print(\"predict (before training)\",  4, forward(4).data[0])\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    for x_val, y_val in zip(x_data, y_data):\n",
    "        l = loss(x_val, y_val)\n",
    "        l.backward()\n",
    "        print(\"\\tgrad: \", x_val, y_val, w.grad.data[0])\n",
    "        w.data = w.data - 0.01 * w.grad.data\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w.grad.data.zero_()\n",
    "\n",
    "    print(\"progress:\", epoch, l.data[0])\n",
    "\n",
    "# After training\n",
    "print(\"predict (after training)\", 4, forward(4).data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyper Parameters \n",
    "input_size = 784  # 28x28\n",
    "hidden_size = 500  # hidden layers neurons\n",
    "num_classes = 10   \n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MNIST Dataset\n",
    "train_dataset = dsets.MNIST(root='./data/',\n",
    "                               train=True,\n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data/',\n",
    "                              train=False,\n",
    "                              transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m        MNIST\n",
       "\u001b[0;31mString form:\u001b[0m <torchvision.datasets.mnist.MNIST object at 0x7fc4e79ccb70>\n",
       "\u001b[0;31mLength:\u001b[0m      60000\n",
       "\u001b[0;31mFile:\u001b[0m        ~/anaconda3/envs/torch/lib/python3.6/site-packages/torchvision/datasets/mnist.py\n",
       "\u001b[0;31mSource:\u001b[0m     \n",
       "\u001b[0;32mclass\u001b[0m \u001b[0mMNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"`MNIST <http://yann.lecun.com/exdb/mnist/>`_ Dataset.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Args:\u001b[0m\n",
       "\u001b[0;34m        root (string): Root directory of dataset where ``processed/training.pt``\u001b[0m\n",
       "\u001b[0;34m            and  ``processed/test.pt`` exist.\u001b[0m\n",
       "\u001b[0;34m        train (bool, optional): If True, creates dataset from ``training.pt``,\u001b[0m\n",
       "\u001b[0;34m            otherwise from ``test.pt``.\u001b[0m\n",
       "\u001b[0;34m        download (bool, optional): If true, downloads the dataset from the internet and\u001b[0m\n",
       "\u001b[0;34m            puts it in root directory. If dataset is already downloaded, it is not\u001b[0m\n",
       "\u001b[0;34m            downloaded again.\u001b[0m\n",
       "\u001b[0;34m        transform (callable, optional): A function/transform that  takes in an PIL image\u001b[0m\n",
       "\u001b[0;34m            and returns a transformed version. E.g, ``transforms.RandomCrop``\u001b[0m\n",
       "\u001b[0;34m        target_transform (callable, optional): A function/transform that takes in the\u001b[0m\n",
       "\u001b[0;34m            target and transforms it.\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0murls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mraw_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mprocessed_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'processed'\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtraining_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'training.pt'\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtest_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'test.pt'\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m  \u001b[0;31m# training set or test set\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Dataset not found.'\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                               \u001b[0;34m' You can use download=True to download it'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
       "\u001b[0;34m        Args:\u001b[0m\n",
       "\u001b[0;34m            index (int): Index\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        Returns:\u001b[0m\n",
       "\u001b[0;34m            tuple: (image, target) where target is index of the target class.\u001b[0m\n",
       "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# doing this so that it is consistent with all other datasets\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# to return a PIL Image\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_check_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \\\n",
       "            \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Download the MNIST data if it doesn't exist in processed_folder already.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mimport\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# download files\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEEXIST\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Downloading '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.gz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mout_f\u001b[0m\u001b[0;34m,\u001b[0m \\\n",
       "                    \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_f\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mout_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# process and save as torch files\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Processing...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mtraining_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mread_image_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train-images-idx3-ubyte'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mread_label_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train-labels-idx1-ubyte'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mread_image_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m't10k-images-idx3-ubyte'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mread_label_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m't10k-labels-idx1-ubyte'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "??train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0mdefault_collate\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0x7fc4e7a7b378\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker_init_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m        \n",
       "\u001b[0;32mclass\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"\u001b[0m\n",
       "\u001b[0;34m    Data loader. Combines a dataset and a sampler, and provides\u001b[0m\n",
       "\u001b[0;34m    single- or multi-process iterators over the dataset.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Arguments:\u001b[0m\n",
       "\u001b[0;34m        dataset (Dataset): dataset from which to load the data.\u001b[0m\n",
       "\u001b[0;34m        batch_size (int, optional): how many samples per batch to load\u001b[0m\n",
       "\u001b[0;34m            (default: 1).\u001b[0m\n",
       "\u001b[0;34m        shuffle (bool, optional): set to ``True`` to have the data reshuffled\u001b[0m\n",
       "\u001b[0;34m            at every epoch (default: False).\u001b[0m\n",
       "\u001b[0;34m        sampler (Sampler, optional): defines the strategy to draw samples from\u001b[0m\n",
       "\u001b[0;34m            the dataset. If specified, ``shuffle`` must be False.\u001b[0m\n",
       "\u001b[0;34m        batch_sampler (Sampler, optional): like sampler, but returns a batch of\u001b[0m\n",
       "\u001b[0;34m            indices at a time. Mutually exclusive with batch_size, shuffle,\u001b[0m\n",
       "\u001b[0;34m            sampler, and drop_last.\u001b[0m\n",
       "\u001b[0;34m        num_workers (int, optional): how many subprocesses to use for data\u001b[0m\n",
       "\u001b[0;34m            loading. 0 means that the data will be loaded in the main process.\u001b[0m\n",
       "\u001b[0;34m            (default: 0)\u001b[0m\n",
       "\u001b[0;34m        collate_fn (callable, optional): merges a list of samples to form a mini-batch.\u001b[0m\n",
       "\u001b[0;34m        pin_memory (bool, optional): If ``True``, the data loader will copy tensors\u001b[0m\n",
       "\u001b[0;34m            into CUDA pinned memory before returning them.\u001b[0m\n",
       "\u001b[0;34m        drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,\u001b[0m\n",
       "\u001b[0;34m            if the dataset size is not divisible by the batch size. If ``False`` and\u001b[0m\n",
       "\u001b[0;34m            the size of dataset is not divisible by the batch size, then the last batch\u001b[0m\n",
       "\u001b[0;34m            will be smaller. (default: False)\u001b[0m\n",
       "\u001b[0;34m        timeout (numeric, optional): if positive, the timeout value for collecting a batch\u001b[0m\n",
       "\u001b[0;34m            from workers. Should always be non-negative. (default: 0)\u001b[0m\n",
       "\u001b[0;34m        worker_init_fn (callable, optional): If not None, this will be called on each\u001b[0m\n",
       "\u001b[0;34m            worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as\u001b[0m\n",
       "\u001b[0;34m            input, after seeding and before data loading. (default: None)\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    .. note:: By default, each worker will have its PyTorch seed set to\u001b[0m\n",
       "\u001b[0;34m              ``base_seed + worker_id``, where ``base_seed`` is a long generated\u001b[0m\n",
       "\u001b[0;34m              by main process using its RNG. You may use ``torch.initial_seed()`` to access\u001b[0m\n",
       "\u001b[0;34m              this value in :attr:`worker_init_fn`, which can be used to set other seeds\u001b[0m\n",
       "\u001b[0;34m              (e.g. NumPy) before data loading.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    .. warning:: If ``spawn'' start method is used, :attr:`worker_init_fn` cannot be an\u001b[0m\n",
       "\u001b[0;34m                 unpicklable object, e.g., a lambda function.\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                 \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                 \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker_init_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_last\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworker_init_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworker_init_fn\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timeout option should be non-negative'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mbatch_sampler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msampler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'batch_sampler is mutually exclusive with '\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                                 \u001b[0;34m'batch_size, shuffle, sampler, and drop_last'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0msampler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sampler is mutually exclusive with shuffle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'num_workers cannot be negative; '\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                             \u001b[0;34m'use num_workers=0 to disable multiprocessing.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mbatch_sampler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0msampler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                    \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                    \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mbatch_sampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_sampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_sampler\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_sampler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m           ~/anaconda3/envs/torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "??torch.utils.data.DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m        DataLoader\n",
       "\u001b[0;31mString form:\u001b[0m <torch.utils.data.dataloader.DataLoader object at 0x7fc4e7c5ae80>\n",
       "\u001b[0;31mLength:\u001b[0m      600\n",
       "\u001b[0;31mFile:\u001b[0m        ~/anaconda3/envs/torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\n",
       "\u001b[0;31mSource:\u001b[0m     \n",
       "\u001b[0;32mclass\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"\u001b[0m\n",
       "\u001b[0;34m    Data loader. Combines a dataset and a sampler, and provides\u001b[0m\n",
       "\u001b[0;34m    single- or multi-process iterators over the dataset.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Arguments:\u001b[0m\n",
       "\u001b[0;34m        dataset (Dataset): dataset from which to load the data.\u001b[0m\n",
       "\u001b[0;34m        batch_size (int, optional): how many samples per batch to load\u001b[0m\n",
       "\u001b[0;34m            (default: 1).\u001b[0m\n",
       "\u001b[0;34m        shuffle (bool, optional): set to ``True`` to have the data reshuffled\u001b[0m\n",
       "\u001b[0;34m            at every epoch (default: False).\u001b[0m\n",
       "\u001b[0;34m        sampler (Sampler, optional): defines the strategy to draw samples from\u001b[0m\n",
       "\u001b[0;34m            the dataset. If specified, ``shuffle`` must be False.\u001b[0m\n",
       "\u001b[0;34m        batch_sampler (Sampler, optional): like sampler, but returns a batch of\u001b[0m\n",
       "\u001b[0;34m            indices at a time. Mutually exclusive with batch_size, shuffle,\u001b[0m\n",
       "\u001b[0;34m            sampler, and drop_last.\u001b[0m\n",
       "\u001b[0;34m        num_workers (int, optional): how many subprocesses to use for data\u001b[0m\n",
       "\u001b[0;34m            loading. 0 means that the data will be loaded in the main process.\u001b[0m\n",
       "\u001b[0;34m            (default: 0)\u001b[0m\n",
       "\u001b[0;34m        collate_fn (callable, optional): merges a list of samples to form a mini-batch.\u001b[0m\n",
       "\u001b[0;34m        pin_memory (bool, optional): If ``True``, the data loader will copy tensors\u001b[0m\n",
       "\u001b[0;34m            into CUDA pinned memory before returning them.\u001b[0m\n",
       "\u001b[0;34m        drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,\u001b[0m\n",
       "\u001b[0;34m            if the dataset size is not divisible by the batch size. If ``False`` and\u001b[0m\n",
       "\u001b[0;34m            the size of dataset is not divisible by the batch size, then the last batch\u001b[0m\n",
       "\u001b[0;34m            will be smaller. (default: False)\u001b[0m\n",
       "\u001b[0;34m        timeout (numeric, optional): if positive, the timeout value for collecting a batch\u001b[0m\n",
       "\u001b[0;34m            from workers. Should always be non-negative. (default: 0)\u001b[0m\n",
       "\u001b[0;34m        worker_init_fn (callable, optional): If not None, this will be called on each\u001b[0m\n",
       "\u001b[0;34m            worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as\u001b[0m\n",
       "\u001b[0;34m            input, after seeding and before data loading. (default: None)\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    .. note:: By default, each worker will have its PyTorch seed set to\u001b[0m\n",
       "\u001b[0;34m              ``base_seed + worker_id``, where ``base_seed`` is a long generated\u001b[0m\n",
       "\u001b[0;34m              by main process using its RNG. You may use ``torch.initial_seed()`` to access\u001b[0m\n",
       "\u001b[0;34m              this value in :attr:`worker_init_fn`, which can be used to set other seeds\u001b[0m\n",
       "\u001b[0;34m              (e.g. NumPy) before data loading.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    .. warning:: If ``spawn'' start method is used, :attr:`worker_init_fn` cannot be an\u001b[0m\n",
       "\u001b[0;34m                 unpicklable object, e.g., a lambda function.\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                 \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                 \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker_init_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_last\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworker_init_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworker_init_fn\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timeout option should be non-negative'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mbatch_sampler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msampler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'batch_sampler is mutually exclusive with '\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                                 \u001b[0;34m'batch_size, shuffle, sampler, and drop_last'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0msampler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sampler is mutually exclusive with shuffle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'num_workers cannot be negative; '\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                             \u001b[0;34m'use num_workers=0 to disable multiprocessing.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mbatch_sampler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0msampler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                    \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                    \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mbatch_sampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_sampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_sampler\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_sampler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "??train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.module.Module"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.mp = nn.MaxPool2d(2)\n",
    "        self.fc = nn.Linear(320, 10)\n",
    "        self.bn = nn.BatchNorm2d()\n",
    "        self.dp = nn.Dropout(0.5)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        x = F.relu(self.mp(self.bnself.conv1(x)))\n",
    "        x = F.relu(self.mp(self.conv2(x)))\n",
    "        x = x.view(in_size, -1)  # flatten the tensor\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0]\n",
    "        # get the index of the max log-probability\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashish/anaconda3/envs/torch/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.299575\n",
      "Train Epoch: 1 [1000/60000 (2%)]\tLoss: 0.876924\n",
      "Train Epoch: 1 [2000/60000 (3%)]\tLoss: 0.611606\n",
      "Train Epoch: 1 [3000/60000 (5%)]\tLoss: 0.326993\n",
      "Train Epoch: 1 [4000/60000 (7%)]\tLoss: 0.331908\n",
      "Train Epoch: 1 [5000/60000 (8%)]\tLoss: 0.152057\n",
      "Train Epoch: 1 [6000/60000 (10%)]\tLoss: 0.073871\n",
      "Train Epoch: 1 [7000/60000 (12%)]\tLoss: 0.266959\n",
      "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 0.097684\n",
      "Train Epoch: 1 [9000/60000 (15%)]\tLoss: 0.100639\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 0.238185\n",
      "Train Epoch: 1 [11000/60000 (18%)]\tLoss: 0.107503\n",
      "Train Epoch: 1 [12000/60000 (20%)]\tLoss: 0.246215\n",
      "Train Epoch: 1 [13000/60000 (22%)]\tLoss: 0.271965\n",
      "Train Epoch: 1 [14000/60000 (23%)]\tLoss: 0.187184\n",
      "Train Epoch: 1 [15000/60000 (25%)]\tLoss: 0.039437\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.149585\n",
      "Train Epoch: 1 [17000/60000 (28%)]\tLoss: 0.106817\n",
      "Train Epoch: 1 [18000/60000 (30%)]\tLoss: 0.101848\n",
      "Train Epoch: 1 [19000/60000 (32%)]\tLoss: 0.050964\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 0.158525\n",
      "Train Epoch: 1 [21000/60000 (35%)]\tLoss: 0.101544\n",
      "Train Epoch: 1 [22000/60000 (37%)]\tLoss: 0.025054\n",
      "Train Epoch: 1 [23000/60000 (38%)]\tLoss: 0.152203\n",
      "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 0.081849\n",
      "Train Epoch: 1 [25000/60000 (42%)]\tLoss: 0.088903\n",
      "Train Epoch: 1 [26000/60000 (43%)]\tLoss: 0.066144\n",
      "Train Epoch: 1 [27000/60000 (45%)]\tLoss: 0.039100\n",
      "Train Epoch: 1 [28000/60000 (47%)]\tLoss: 0.055081\n",
      "Train Epoch: 1 [29000/60000 (48%)]\tLoss: 0.046907\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 0.152598\n",
      "Train Epoch: 1 [31000/60000 (52%)]\tLoss: 0.028240\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.123775\n",
      "Train Epoch: 1 [33000/60000 (55%)]\tLoss: 0.095422\n",
      "Train Epoch: 1 [34000/60000 (57%)]\tLoss: 0.091690\n",
      "Train Epoch: 1 [35000/60000 (58%)]\tLoss: 0.071662\n",
      "Train Epoch: 1 [36000/60000 (60%)]\tLoss: 0.085413\n",
      "Train Epoch: 1 [37000/60000 (62%)]\tLoss: 0.181881\n",
      "Train Epoch: 1 [38000/60000 (63%)]\tLoss: 0.018563\n",
      "Train Epoch: 1 [39000/60000 (65%)]\tLoss: 0.039402\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.113454\n",
      "Train Epoch: 1 [41000/60000 (68%)]\tLoss: 0.056702\n",
      "Train Epoch: 1 [42000/60000 (70%)]\tLoss: 0.079526\n",
      "Train Epoch: 1 [43000/60000 (72%)]\tLoss: 0.101662\n",
      "Train Epoch: 1 [44000/60000 (73%)]\tLoss: 0.114677\n",
      "Train Epoch: 1 [45000/60000 (75%)]\tLoss: 0.105864\n",
      "Train Epoch: 1 [46000/60000 (77%)]\tLoss: 0.089832\n",
      "Train Epoch: 1 [47000/60000 (78%)]\tLoss: 0.137000\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.109770\n",
      "Train Epoch: 1 [49000/60000 (82%)]\tLoss: 0.082116\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 0.031173\n",
      "Train Epoch: 1 [51000/60000 (85%)]\tLoss: 0.035028\n",
      "Train Epoch: 1 [52000/60000 (87%)]\tLoss: 0.058478\n",
      "Train Epoch: 1 [53000/60000 (88%)]\tLoss: 0.101842\n",
      "Train Epoch: 1 [54000/60000 (90%)]\tLoss: 0.115702\n",
      "Train Epoch: 1 [55000/60000 (92%)]\tLoss: 0.045100\n",
      "Train Epoch: 1 [56000/60000 (93%)]\tLoss: 0.050643\n",
      "Train Epoch: 1 [57000/60000 (95%)]\tLoss: 0.038308\n",
      "Train Epoch: 1 [58000/60000 (97%)]\tLoss: 0.025532\n",
      "Train Epoch: 1 [59000/60000 (98%)]\tLoss: 0.001952\n",
      "\n",
      "Test set: Average loss: 0.0761, Accuracy: 9766/10000 (98%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.051918\n",
      "Train Epoch: 2 [1000/60000 (2%)]\tLoss: 0.118014\n",
      "Train Epoch: 2 [2000/60000 (3%)]\tLoss: 0.094562\n",
      "Train Epoch: 2 [3000/60000 (5%)]\tLoss: 0.045117\n",
      "Train Epoch: 2 [4000/60000 (7%)]\tLoss: 0.062483\n",
      "Train Epoch: 2 [5000/60000 (8%)]\tLoss: 0.032327\n",
      "Train Epoch: 2 [6000/60000 (10%)]\tLoss: 0.054181\n",
      "Train Epoch: 2 [7000/60000 (12%)]\tLoss: 0.199204\n",
      "Train Epoch: 2 [8000/60000 (13%)]\tLoss: 0.041650\n",
      "Train Epoch: 2 [9000/60000 (15%)]\tLoss: 0.070106\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 0.105326\n",
      "Train Epoch: 2 [11000/60000 (18%)]\tLoss: 0.079654\n",
      "Train Epoch: 2 [12000/60000 (20%)]\tLoss: 0.056282\n",
      "Train Epoch: 2 [13000/60000 (22%)]\tLoss: 0.109028\n",
      "Train Epoch: 2 [14000/60000 (23%)]\tLoss: 0.056331\n",
      "Train Epoch: 2 [15000/60000 (25%)]\tLoss: 0.011790\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.042648\n",
      "Train Epoch: 2 [17000/60000 (28%)]\tLoss: 0.056274\n",
      "Train Epoch: 2 [18000/60000 (30%)]\tLoss: 0.051643\n",
      "Train Epoch: 2 [19000/60000 (32%)]\tLoss: 0.041578\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.035239\n",
      "Train Epoch: 2 [21000/60000 (35%)]\tLoss: 0.059597\n",
      "Train Epoch: 2 [22000/60000 (37%)]\tLoss: 0.004579\n",
      "Train Epoch: 2 [23000/60000 (38%)]\tLoss: 0.137191\n",
      "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 0.029898\n",
      "Train Epoch: 2 [25000/60000 (42%)]\tLoss: 0.028670\n",
      "Train Epoch: 2 [26000/60000 (43%)]\tLoss: 0.022141\n",
      "Train Epoch: 2 [27000/60000 (45%)]\tLoss: 0.058766\n",
      "Train Epoch: 2 [28000/60000 (47%)]\tLoss: 0.047330\n",
      "Train Epoch: 2 [29000/60000 (48%)]\tLoss: 0.032160\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 0.120848\n",
      "Train Epoch: 2 [31000/60000 (52%)]\tLoss: 0.072133\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.063304\n",
      "Train Epoch: 2 [33000/60000 (55%)]\tLoss: 0.037986\n",
      "Train Epoch: 2 [34000/60000 (57%)]\tLoss: 0.066247\n",
      "Train Epoch: 2 [35000/60000 (58%)]\tLoss: 0.070449\n",
      "Train Epoch: 2 [36000/60000 (60%)]\tLoss: 0.027628\n",
      "Train Epoch: 2 [37000/60000 (62%)]\tLoss: 0.126451\n",
      "Train Epoch: 2 [38000/60000 (63%)]\tLoss: 0.013668\n",
      "Train Epoch: 2 [39000/60000 (65%)]\tLoss: 0.024311\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.042214\n",
      "Train Epoch: 2 [41000/60000 (68%)]\tLoss: 0.050576\n",
      "Train Epoch: 2 [42000/60000 (70%)]\tLoss: 0.045691\n",
      "Train Epoch: 2 [43000/60000 (72%)]\tLoss: 0.060384\n",
      "Train Epoch: 2 [44000/60000 (73%)]\tLoss: 0.092812\n",
      "Train Epoch: 2 [45000/60000 (75%)]\tLoss: 0.041691\n",
      "Train Epoch: 2 [46000/60000 (77%)]\tLoss: 0.064217\n",
      "Train Epoch: 2 [47000/60000 (78%)]\tLoss: 0.085609\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.082871\n",
      "Train Epoch: 2 [49000/60000 (82%)]\tLoss: 0.146862\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 0.019660\n",
      "Train Epoch: 2 [51000/60000 (85%)]\tLoss: 0.001865\n",
      "Train Epoch: 2 [52000/60000 (87%)]\tLoss: 0.022236\n",
      "Train Epoch: 2 [53000/60000 (88%)]\tLoss: 0.052781\n",
      "Train Epoch: 2 [54000/60000 (90%)]\tLoss: 0.075753\n",
      "Train Epoch: 2 [55000/60000 (92%)]\tLoss: 0.010361\n",
      "Train Epoch: 2 [56000/60000 (93%)]\tLoss: 0.036250\n",
      "Train Epoch: 2 [57000/60000 (95%)]\tLoss: 0.018153\n",
      "Train Epoch: 2 [58000/60000 (97%)]\tLoss: 0.049544\n",
      "Train Epoch: 2 [59000/60000 (98%)]\tLoss: 0.000286\n",
      "\n",
      "Test set: Average loss: 0.0664, Accuracy: 9805/10000 (98%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.030122\n",
      "Train Epoch: 3 [1000/60000 (2%)]\tLoss: 0.031852\n",
      "Train Epoch: 3 [2000/60000 (3%)]\tLoss: 0.051453\n",
      "Train Epoch: 3 [3000/60000 (5%)]\tLoss: 0.018756\n",
      "Train Epoch: 3 [4000/60000 (7%)]\tLoss: 0.028670\n",
      "Train Epoch: 3 [5000/60000 (8%)]\tLoss: 0.021394\n",
      "Train Epoch: 3 [6000/60000 (10%)]\tLoss: 0.032893\n",
      "Train Epoch: 3 [7000/60000 (12%)]\tLoss: 0.143197\n",
      "Train Epoch: 3 [8000/60000 (13%)]\tLoss: 0.039052\n",
      "Train Epoch: 3 [9000/60000 (15%)]\tLoss: 0.047649\n",
      "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 0.091340\n",
      "Train Epoch: 3 [11000/60000 (18%)]\tLoss: 0.019208\n",
      "Train Epoch: 3 [12000/60000 (20%)]\tLoss: 0.064972\n",
      "Train Epoch: 3 [13000/60000 (22%)]\tLoss: 0.088570\n",
      "Train Epoch: 3 [14000/60000 (23%)]\tLoss: 0.055742\n",
      "Train Epoch: 3 [15000/60000 (25%)]\tLoss: 0.024132\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.033282\n",
      "Train Epoch: 3 [17000/60000 (28%)]\tLoss: 0.072125\n",
      "Train Epoch: 3 [18000/60000 (30%)]\tLoss: 0.082789\n",
      "Train Epoch: 3 [19000/60000 (32%)]\tLoss: 0.044242\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 0.042173\n",
      "Train Epoch: 3 [21000/60000 (35%)]\tLoss: 0.030003\n",
      "Train Epoch: 3 [22000/60000 (37%)]\tLoss: 0.002324\n",
      "Train Epoch: 3 [23000/60000 (38%)]\tLoss: 0.077288\n",
      "Train Epoch: 3 [24000/60000 (40%)]\tLoss: 0.025697\n",
      "Train Epoch: 3 [25000/60000 (42%)]\tLoss: 0.023509\n",
      "Train Epoch: 3 [26000/60000 (43%)]\tLoss: 0.015759\n",
      "Train Epoch: 3 [27000/60000 (45%)]\tLoss: 0.010073\n",
      "Train Epoch: 3 [28000/60000 (47%)]\tLoss: 0.022821\n",
      "Train Epoch: 3 [29000/60000 (48%)]\tLoss: 0.024787\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 0.139225\n",
      "Train Epoch: 3 [31000/60000 (52%)]\tLoss: 0.070411\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.033214\n",
      "Train Epoch: 3 [33000/60000 (55%)]\tLoss: 0.011593\n",
      "Train Epoch: 3 [34000/60000 (57%)]\tLoss: 0.081489\n",
      "Train Epoch: 3 [35000/60000 (58%)]\tLoss: 0.076384\n",
      "Train Epoch: 3 [36000/60000 (60%)]\tLoss: 0.042981\n",
      "Train Epoch: 3 [37000/60000 (62%)]\tLoss: 0.124333\n",
      "Train Epoch: 3 [38000/60000 (63%)]\tLoss: 0.020214\n",
      "Train Epoch: 3 [39000/60000 (65%)]\tLoss: 0.039918\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.036660\n",
      "Train Epoch: 3 [41000/60000 (68%)]\tLoss: 0.027864\n",
      "Train Epoch: 3 [42000/60000 (70%)]\tLoss: 0.081415\n",
      "Train Epoch: 3 [43000/60000 (72%)]\tLoss: 0.060547\n",
      "Train Epoch: 3 [44000/60000 (73%)]\tLoss: 0.044785\n",
      "Train Epoch: 3 [45000/60000 (75%)]\tLoss: 0.041825\n",
      "Train Epoch: 3 [46000/60000 (77%)]\tLoss: 0.083220\n",
      "Train Epoch: 3 [47000/60000 (78%)]\tLoss: 0.076847\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.075927\n",
      "Train Epoch: 3 [49000/60000 (82%)]\tLoss: 0.095076\n",
      "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 0.018210\n",
      "Train Epoch: 3 [51000/60000 (85%)]\tLoss: 0.002027\n",
      "Train Epoch: 3 [52000/60000 (87%)]\tLoss: 0.008825\n",
      "Train Epoch: 3 [53000/60000 (88%)]\tLoss: 0.064682\n",
      "Train Epoch: 3 [54000/60000 (90%)]\tLoss: 0.038600\n",
      "Train Epoch: 3 [55000/60000 (92%)]\tLoss: 0.014178\n",
      "Train Epoch: 3 [56000/60000 (93%)]\tLoss: 0.053180\n",
      "Train Epoch: 3 [57000/60000 (95%)]\tLoss: 0.030069\n",
      "Train Epoch: 3 [58000/60000 (97%)]\tLoss: 0.027850\n",
      "Train Epoch: 3 [59000/60000 (98%)]\tLoss: 0.000364\n",
      "\n",
      "Test set: Average loss: 0.0460, Accuracy: 9845/10000 (98%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.011230\n",
      "Train Epoch: 4 [1000/60000 (2%)]\tLoss: 0.037555\n",
      "Train Epoch: 4 [2000/60000 (3%)]\tLoss: 0.079186\n",
      "Train Epoch: 4 [3000/60000 (5%)]\tLoss: 0.027206\n",
      "Train Epoch: 4 [4000/60000 (7%)]\tLoss: 0.020221\n",
      "Train Epoch: 4 [5000/60000 (8%)]\tLoss: 0.009071\n",
      "Train Epoch: 4 [6000/60000 (10%)]\tLoss: 0.035720\n",
      "Train Epoch: 4 [7000/60000 (12%)]\tLoss: 0.143610\n",
      "Train Epoch: 4 [8000/60000 (13%)]\tLoss: 0.020071\n",
      "Train Epoch: 4 [9000/60000 (15%)]\tLoss: 0.027637\n",
      "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 0.006957\n",
      "Train Epoch: 4 [11000/60000 (18%)]\tLoss: 0.086923\n",
      "Train Epoch: 4 [12000/60000 (20%)]\tLoss: 0.074537\n",
      "Train Epoch: 4 [13000/60000 (22%)]\tLoss: 0.107041\n",
      "Train Epoch: 4 [14000/60000 (23%)]\tLoss: 0.014855\n",
      "Train Epoch: 4 [15000/60000 (25%)]\tLoss: 0.011432\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.026924\n",
      "Train Epoch: 4 [17000/60000 (28%)]\tLoss: 0.029594\n",
      "Train Epoch: 4 [18000/60000 (30%)]\tLoss: 0.019117\n",
      "Train Epoch: 4 [19000/60000 (32%)]\tLoss: 0.046202\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 0.021951\n",
      "Train Epoch: 4 [21000/60000 (35%)]\tLoss: 0.007480\n",
      "Train Epoch: 4 [22000/60000 (37%)]\tLoss: 0.007644\n",
      "Train Epoch: 4 [23000/60000 (38%)]\tLoss: 0.077284\n",
      "Train Epoch: 4 [24000/60000 (40%)]\tLoss: 0.028524\n",
      "Train Epoch: 4 [25000/60000 (42%)]\tLoss: 0.027755\n",
      "Train Epoch: 4 [26000/60000 (43%)]\tLoss: 0.008693\n",
      "Train Epoch: 4 [27000/60000 (45%)]\tLoss: 0.033287\n",
      "Train Epoch: 4 [28000/60000 (47%)]\tLoss: 0.026468\n",
      "Train Epoch: 4 [29000/60000 (48%)]\tLoss: 0.046488\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 0.146014\n",
      "Train Epoch: 4 [31000/60000 (52%)]\tLoss: 0.045939\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.016628\n",
      "Train Epoch: 4 [33000/60000 (55%)]\tLoss: 0.010135\n",
      "Train Epoch: 4 [34000/60000 (57%)]\tLoss: 0.035102\n",
      "Train Epoch: 4 [35000/60000 (58%)]\tLoss: 0.008044\n",
      "Train Epoch: 4 [36000/60000 (60%)]\tLoss: 0.077232\n",
      "Train Epoch: 4 [37000/60000 (62%)]\tLoss: 0.177815\n",
      "Train Epoch: 4 [38000/60000 (63%)]\tLoss: 0.008338\n",
      "Train Epoch: 4 [39000/60000 (65%)]\tLoss: 0.004285\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 0.007372\n",
      "Train Epoch: 4 [41000/60000 (68%)]\tLoss: 0.038955\n",
      "Train Epoch: 4 [42000/60000 (70%)]\tLoss: 0.029095\n",
      "Train Epoch: 4 [43000/60000 (72%)]\tLoss: 0.026129\n",
      "Train Epoch: 4 [44000/60000 (73%)]\tLoss: 0.025243\n",
      "Train Epoch: 4 [45000/60000 (75%)]\tLoss: 0.023034\n",
      "Train Epoch: 4 [46000/60000 (77%)]\tLoss: 0.075764\n",
      "Train Epoch: 4 [47000/60000 (78%)]\tLoss: 0.043989\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.045702\n",
      "Train Epoch: 4 [49000/60000 (82%)]\tLoss: 0.123456\n",
      "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 0.007665\n",
      "Train Epoch: 4 [51000/60000 (85%)]\tLoss: 0.003929\n",
      "Train Epoch: 4 [52000/60000 (87%)]\tLoss: 0.002815\n",
      "Train Epoch: 4 [53000/60000 (88%)]\tLoss: 0.062396\n",
      "Train Epoch: 4 [54000/60000 (90%)]\tLoss: 0.029245\n",
      "Train Epoch: 4 [55000/60000 (92%)]\tLoss: 0.016686\n",
      "Train Epoch: 4 [56000/60000 (93%)]\tLoss: 0.055153\n",
      "Train Epoch: 4 [57000/60000 (95%)]\tLoss: 0.023763\n",
      "Train Epoch: 4 [58000/60000 (97%)]\tLoss: 0.035677\n",
      "Train Epoch: 4 [59000/60000 (98%)]\tLoss: 0.000379\n",
      "\n",
      "Test set: Average loss: 0.0570, Accuracy: 9841/10000 (98%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.011156\n",
      "Train Epoch: 5 [1000/60000 (2%)]\tLoss: 0.069535\n",
      "Train Epoch: 5 [2000/60000 (3%)]\tLoss: 0.027677\n",
      "Train Epoch: 5 [3000/60000 (5%)]\tLoss: 0.040910\n",
      "Train Epoch: 5 [4000/60000 (7%)]\tLoss: 0.027603\n",
      "Train Epoch: 5 [5000/60000 (8%)]\tLoss: 0.022749\n",
      "Train Epoch: 5 [6000/60000 (10%)]\tLoss: 0.054632\n",
      "Train Epoch: 5 [7000/60000 (12%)]\tLoss: 0.179975\n",
      "Train Epoch: 5 [8000/60000 (13%)]\tLoss: 0.001745\n",
      "Train Epoch: 5 [9000/60000 (15%)]\tLoss: 0.021216\n",
      "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 0.097408\n",
      "Train Epoch: 5 [11000/60000 (18%)]\tLoss: 0.100476\n",
      "Train Epoch: 5 [12000/60000 (20%)]\tLoss: 0.074918\n",
      "Train Epoch: 5 [13000/60000 (22%)]\tLoss: 0.026834\n",
      "Train Epoch: 5 [14000/60000 (23%)]\tLoss: 0.075056\n",
      "Train Epoch: 5 [15000/60000 (25%)]\tLoss: 0.000639\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.025907\n",
      "Train Epoch: 5 [17000/60000 (28%)]\tLoss: 0.015524\n",
      "Train Epoch: 5 [18000/60000 (30%)]\tLoss: 0.017040\n",
      "Train Epoch: 5 [19000/60000 (32%)]\tLoss: 0.015364\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 0.039446\n",
      "Train Epoch: 5 [21000/60000 (35%)]\tLoss: 0.048198\n",
      "Train Epoch: 5 [22000/60000 (37%)]\tLoss: 0.001520\n",
      "Train Epoch: 5 [23000/60000 (38%)]\tLoss: 0.013680\n",
      "Train Epoch: 5 [24000/60000 (40%)]\tLoss: 0.008430\n",
      "Train Epoch: 5 [25000/60000 (42%)]\tLoss: 0.013524\n",
      "Train Epoch: 5 [26000/60000 (43%)]\tLoss: 0.024092\n",
      "Train Epoch: 5 [27000/60000 (45%)]\tLoss: 0.020014\n",
      "Train Epoch: 5 [28000/60000 (47%)]\tLoss: 0.059778\n",
      "Train Epoch: 5 [29000/60000 (48%)]\tLoss: 0.007419\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 0.191279\n",
      "Train Epoch: 5 [31000/60000 (52%)]\tLoss: 0.049931\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.037263\n",
      "Train Epoch: 5 [33000/60000 (55%)]\tLoss: 0.007445\n",
      "Train Epoch: 5 [34000/60000 (57%)]\tLoss: 0.021017\n",
      "Train Epoch: 5 [35000/60000 (58%)]\tLoss: 0.068715\n",
      "Train Epoch: 5 [36000/60000 (60%)]\tLoss: 0.032201\n",
      "Train Epoch: 5 [37000/60000 (62%)]\tLoss: 0.167351\n",
      "Train Epoch: 5 [38000/60000 (63%)]\tLoss: 0.003401\n",
      "Train Epoch: 5 [39000/60000 (65%)]\tLoss: 0.044257\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 0.009411\n",
      "Train Epoch: 5 [41000/60000 (68%)]\tLoss: 0.019346\n",
      "Train Epoch: 5 [42000/60000 (70%)]\tLoss: 0.076086\n",
      "Train Epoch: 5 [43000/60000 (72%)]\tLoss: 0.079463\n",
      "Train Epoch: 5 [44000/60000 (73%)]\tLoss: 0.061466\n",
      "Train Epoch: 5 [45000/60000 (75%)]\tLoss: 0.014692\n",
      "Train Epoch: 5 [46000/60000 (77%)]\tLoss: 0.072186\n",
      "Train Epoch: 5 [47000/60000 (78%)]\tLoss: 0.050063\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.023345\n",
      "Train Epoch: 5 [49000/60000 (82%)]\tLoss: 0.085223\n",
      "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 0.003405\n",
      "Train Epoch: 5 [51000/60000 (85%)]\tLoss: 0.002435\n",
      "Train Epoch: 5 [52000/60000 (87%)]\tLoss: 0.000966\n",
      "Train Epoch: 5 [53000/60000 (88%)]\tLoss: 0.047415\n",
      "Train Epoch: 5 [54000/60000 (90%)]\tLoss: 0.022919\n",
      "Train Epoch: 5 [55000/60000 (92%)]\tLoss: 0.007002\n",
      "Train Epoch: 5 [56000/60000 (93%)]\tLoss: 0.100008\n",
      "Train Epoch: 5 [57000/60000 (95%)]\tLoss: 0.037872\n",
      "Train Epoch: 5 [58000/60000 (97%)]\tLoss: 0.014661\n",
      "Train Epoch: 5 [59000/60000 (98%)]\tLoss: 0.000024\n",
      "\n",
      "Test set: Average loss: 0.0467, Accuracy: 9853/10000 (99%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.066449\n",
      "Train Epoch: 6 [1000/60000 (2%)]\tLoss: 0.032881\n",
      "Train Epoch: 6 [2000/60000 (3%)]\tLoss: 0.030733\n",
      "Train Epoch: 6 [3000/60000 (5%)]\tLoss: 0.032207\n",
      "Train Epoch: 6 [4000/60000 (7%)]\tLoss: 0.045856\n",
      "Train Epoch: 6 [5000/60000 (8%)]\tLoss: 0.053431\n",
      "Train Epoch: 6 [6000/60000 (10%)]\tLoss: 0.065722\n",
      "Train Epoch: 6 [7000/60000 (12%)]\tLoss: 0.191806\n",
      "Train Epoch: 6 [8000/60000 (13%)]\tLoss: 0.010554\n",
      "Train Epoch: 6 [9000/60000 (15%)]\tLoss: 0.017356\n",
      "Train Epoch: 6 [10000/60000 (17%)]\tLoss: 0.054424\n",
      "Train Epoch: 6 [11000/60000 (18%)]\tLoss: 0.000977\n",
      "Train Epoch: 6 [12000/60000 (20%)]\tLoss: 0.032608\n",
      "Train Epoch: 6 [13000/60000 (22%)]\tLoss: 0.090912\n",
      "Train Epoch: 6 [14000/60000 (23%)]\tLoss: 0.042759\n",
      "Train Epoch: 6 [15000/60000 (25%)]\tLoss: 0.000404\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.007036\n",
      "Train Epoch: 6 [17000/60000 (28%)]\tLoss: 0.018660\n",
      "Train Epoch: 6 [18000/60000 (30%)]\tLoss: 0.046464\n",
      "Train Epoch: 6 [19000/60000 (32%)]\tLoss: 0.004372\n",
      "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 0.136782\n",
      "Train Epoch: 6 [21000/60000 (35%)]\tLoss: 0.031685\n",
      "Train Epoch: 6 [22000/60000 (37%)]\tLoss: 0.000755\n",
      "Train Epoch: 6 [23000/60000 (38%)]\tLoss: 0.051006\n",
      "Train Epoch: 6 [24000/60000 (40%)]\tLoss: 0.021108\n",
      "Train Epoch: 6 [25000/60000 (42%)]\tLoss: 0.015211\n",
      "Train Epoch: 6 [26000/60000 (43%)]\tLoss: 0.001797\n",
      "Train Epoch: 6 [27000/60000 (45%)]\tLoss: 0.006962\n",
      "Train Epoch: 6 [28000/60000 (47%)]\tLoss: 0.045039\n",
      "Train Epoch: 6 [29000/60000 (48%)]\tLoss: 0.029259\n",
      "Train Epoch: 6 [30000/60000 (50%)]\tLoss: 0.114660\n",
      "Train Epoch: 6 [31000/60000 (52%)]\tLoss: 0.045551\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.001192\n",
      "Train Epoch: 6 [33000/60000 (55%)]\tLoss: 0.007048\n",
      "Train Epoch: 6 [34000/60000 (57%)]\tLoss: 0.074746\n",
      "Train Epoch: 6 [35000/60000 (58%)]\tLoss: 0.054516\n",
      "Train Epoch: 6 [36000/60000 (60%)]\tLoss: 0.004165\n",
      "Train Epoch: 6 [37000/60000 (62%)]\tLoss: 0.092197\n",
      "Train Epoch: 6 [38000/60000 (63%)]\tLoss: 0.008735\n",
      "Train Epoch: 6 [39000/60000 (65%)]\tLoss: 0.021029\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 0.056192\n",
      "Train Epoch: 6 [41000/60000 (68%)]\tLoss: 0.027405\n",
      "Train Epoch: 6 [42000/60000 (70%)]\tLoss: 0.091729\n",
      "Train Epoch: 6 [43000/60000 (72%)]\tLoss: 0.056437\n",
      "Train Epoch: 6 [44000/60000 (73%)]\tLoss: 0.034616\n",
      "Train Epoch: 6 [45000/60000 (75%)]\tLoss: 0.062767\n",
      "Train Epoch: 6 [46000/60000 (77%)]\tLoss: 0.107306\n",
      "Train Epoch: 6 [47000/60000 (78%)]\tLoss: 0.030074\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.018183\n",
      "Train Epoch: 6 [49000/60000 (82%)]\tLoss: 0.100492\n",
      "Train Epoch: 6 [50000/60000 (83%)]\tLoss: 0.002668\n",
      "Train Epoch: 6 [51000/60000 (85%)]\tLoss: 0.002150\n",
      "Train Epoch: 6 [52000/60000 (87%)]\tLoss: 0.001855\n",
      "Train Epoch: 6 [53000/60000 (88%)]\tLoss: 0.089841\n",
      "Train Epoch: 6 [54000/60000 (90%)]\tLoss: 0.047387\n",
      "Train Epoch: 6 [55000/60000 (92%)]\tLoss: 0.005154\n",
      "Train Epoch: 6 [56000/60000 (93%)]\tLoss: 0.062047\n",
      "Train Epoch: 6 [57000/60000 (95%)]\tLoss: 0.016825\n",
      "Train Epoch: 6 [58000/60000 (97%)]\tLoss: 0.037347\n",
      "Train Epoch: 6 [59000/60000 (98%)]\tLoss: 0.000136\n",
      "\n",
      "Test set: Average loss: 0.0495, Accuracy: 9874/10000 (99%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.003998\n",
      "Train Epoch: 7 [1000/60000 (2%)]\tLoss: 0.037165\n",
      "Train Epoch: 7 [2000/60000 (3%)]\tLoss: 0.021899\n",
      "Train Epoch: 7 [3000/60000 (5%)]\tLoss: 0.040933\n",
      "Train Epoch: 7 [4000/60000 (7%)]\tLoss: 0.014527\n",
      "Train Epoch: 7 [5000/60000 (8%)]\tLoss: 0.023727\n",
      "Train Epoch: 7 [6000/60000 (10%)]\tLoss: 0.018294\n",
      "Train Epoch: 7 [7000/60000 (12%)]\tLoss: 0.171790\n",
      "Train Epoch: 7 [8000/60000 (13%)]\tLoss: 0.001120\n",
      "Train Epoch: 7 [9000/60000 (15%)]\tLoss: 0.002324\n",
      "Train Epoch: 7 [10000/60000 (17%)]\tLoss: 0.115054\n",
      "Train Epoch: 7 [11000/60000 (18%)]\tLoss: 0.112454\n",
      "Train Epoch: 7 [12000/60000 (20%)]\tLoss: 0.008024\n",
      "Train Epoch: 7 [13000/60000 (22%)]\tLoss: 0.013298\n",
      "Train Epoch: 7 [14000/60000 (23%)]\tLoss: 0.103812\n",
      "Train Epoch: 7 [15000/60000 (25%)]\tLoss: 0.000426\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.049605\n",
      "Train Epoch: 7 [17000/60000 (28%)]\tLoss: 0.021497\n",
      "Train Epoch: 7 [18000/60000 (30%)]\tLoss: 0.056583\n",
      "Train Epoch: 7 [19000/60000 (32%)]\tLoss: 0.032571\n",
      "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 0.098111\n",
      "Train Epoch: 7 [21000/60000 (35%)]\tLoss: 0.011062\n",
      "Train Epoch: 7 [22000/60000 (37%)]\tLoss: 0.002558\n",
      "Train Epoch: 7 [23000/60000 (38%)]\tLoss: 0.045567\n",
      "Train Epoch: 7 [24000/60000 (40%)]\tLoss: 0.009674\n",
      "Train Epoch: 7 [25000/60000 (42%)]\tLoss: 0.003593\n",
      "Train Epoch: 7 [26000/60000 (43%)]\tLoss: 0.000944\n",
      "Train Epoch: 7 [27000/60000 (45%)]\tLoss: 0.026342\n",
      "Train Epoch: 7 [28000/60000 (47%)]\tLoss: 0.054241\n",
      "Train Epoch: 7 [29000/60000 (48%)]\tLoss: 0.035550\n",
      "Train Epoch: 7 [30000/60000 (50%)]\tLoss: 0.043499\n",
      "Train Epoch: 7 [31000/60000 (52%)]\tLoss: 0.031260\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.023974\n",
      "Train Epoch: 7 [33000/60000 (55%)]\tLoss: 0.023607\n",
      "Train Epoch: 7 [34000/60000 (57%)]\tLoss: 0.056407\n",
      "Train Epoch: 7 [35000/60000 (58%)]\tLoss: 0.003372\n",
      "Train Epoch: 7 [36000/60000 (60%)]\tLoss: 0.041809\n",
      "Train Epoch: 7 [37000/60000 (62%)]\tLoss: 0.110580\n",
      "Train Epoch: 7 [38000/60000 (63%)]\tLoss: 0.007546\n",
      "Train Epoch: 7 [39000/60000 (65%)]\tLoss: 0.003620\n",
      "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 0.038838\n",
      "Train Epoch: 7 [41000/60000 (68%)]\tLoss: 0.024640\n",
      "Train Epoch: 7 [42000/60000 (70%)]\tLoss: 0.052945\n",
      "Train Epoch: 7 [43000/60000 (72%)]\tLoss: 0.038118\n",
      "Train Epoch: 7 [44000/60000 (73%)]\tLoss: 0.029139\n",
      "Train Epoch: 7 [45000/60000 (75%)]\tLoss: 0.075162\n",
      "Train Epoch: 7 [46000/60000 (77%)]\tLoss: 0.069028\n",
      "Train Epoch: 7 [47000/60000 (78%)]\tLoss: 0.036332\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.020952\n",
      "Train Epoch: 7 [49000/60000 (82%)]\tLoss: 0.021538\n",
      "Train Epoch: 7 [50000/60000 (83%)]\tLoss: 0.000440\n",
      "Train Epoch: 7 [51000/60000 (85%)]\tLoss: 0.000490\n",
      "Train Epoch: 7 [52000/60000 (87%)]\tLoss: 0.010591\n",
      "Train Epoch: 7 [53000/60000 (88%)]\tLoss: 0.103671\n",
      "Train Epoch: 7 [54000/60000 (90%)]\tLoss: 0.040602\n",
      "Train Epoch: 7 [55000/60000 (92%)]\tLoss: 0.002567\n",
      "Train Epoch: 7 [56000/60000 (93%)]\tLoss: 0.054129\n",
      "Train Epoch: 7 [57000/60000 (95%)]\tLoss: 0.028485\n",
      "Train Epoch: 7 [58000/60000 (97%)]\tLoss: 0.038182\n",
      "Train Epoch: 7 [59000/60000 (98%)]\tLoss: 0.001290\n",
      "\n",
      "Test set: Average loss: 0.0600, Accuracy: 9857/10000 (99%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.041538\n",
      "Train Epoch: 8 [1000/60000 (2%)]\tLoss: 0.116770\n",
      "Train Epoch: 8 [2000/60000 (3%)]\tLoss: 0.043415\n",
      "Train Epoch: 8 [3000/60000 (5%)]\tLoss: 0.014073\n",
      "Train Epoch: 8 [4000/60000 (7%)]\tLoss: 0.038403\n",
      "Train Epoch: 8 [5000/60000 (8%)]\tLoss: 0.039225\n",
      "Train Epoch: 8 [6000/60000 (10%)]\tLoss: 0.011354\n",
      "Train Epoch: 8 [7000/60000 (12%)]\tLoss: 0.071195\n",
      "Train Epoch: 8 [8000/60000 (13%)]\tLoss: 0.000877\n",
      "Train Epoch: 8 [9000/60000 (15%)]\tLoss: 0.005587\n",
      "Train Epoch: 8 [10000/60000 (17%)]\tLoss: 0.023396\n",
      "Train Epoch: 8 [11000/60000 (18%)]\tLoss: 0.062324\n",
      "Train Epoch: 8 [12000/60000 (20%)]\tLoss: 0.023187\n",
      "Train Epoch: 8 [13000/60000 (22%)]\tLoss: 0.049506\n",
      "Train Epoch: 8 [14000/60000 (23%)]\tLoss: 0.054169\n",
      "Train Epoch: 8 [15000/60000 (25%)]\tLoss: 0.012964\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.017462\n",
      "Train Epoch: 8 [17000/60000 (28%)]\tLoss: 0.022346\n",
      "Train Epoch: 8 [18000/60000 (30%)]\tLoss: 0.020329\n",
      "Train Epoch: 8 [19000/60000 (32%)]\tLoss: 0.046682\n",
      "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 0.166419\n",
      "Train Epoch: 8 [21000/60000 (35%)]\tLoss: 0.001615\n",
      "Train Epoch: 8 [22000/60000 (37%)]\tLoss: 0.018554\n",
      "Train Epoch: 8 [23000/60000 (38%)]\tLoss: 0.112281\n",
      "Train Epoch: 8 [24000/60000 (40%)]\tLoss: 0.000346\n",
      "Train Epoch: 8 [25000/60000 (42%)]\tLoss: 0.009538\n",
      "Train Epoch: 8 [26000/60000 (43%)]\tLoss: 0.011832\n",
      "Train Epoch: 8 [27000/60000 (45%)]\tLoss: 0.007517\n",
      "Train Epoch: 8 [28000/60000 (47%)]\tLoss: 0.053335\n",
      "Train Epoch: 8 [29000/60000 (48%)]\tLoss: 0.005055\n",
      "Train Epoch: 8 [30000/60000 (50%)]\tLoss: 0.151280\n",
      "Train Epoch: 8 [31000/60000 (52%)]\tLoss: 0.057621\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.000492\n",
      "Train Epoch: 8 [33000/60000 (55%)]\tLoss: 0.001021\n",
      "Train Epoch: 8 [34000/60000 (57%)]\tLoss: 0.042626\n",
      "Train Epoch: 8 [35000/60000 (58%)]\tLoss: 0.009198\n",
      "Train Epoch: 8 [36000/60000 (60%)]\tLoss: 0.013953\n",
      "Train Epoch: 8 [37000/60000 (62%)]\tLoss: 0.059656\n",
      "Train Epoch: 8 [38000/60000 (63%)]\tLoss: 0.009024\n",
      "Train Epoch: 8 [39000/60000 (65%)]\tLoss: 0.024839\n",
      "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 0.005244\n",
      "Train Epoch: 8 [41000/60000 (68%)]\tLoss: 0.003819\n",
      "Train Epoch: 8 [42000/60000 (70%)]\tLoss: 0.012252\n",
      "Train Epoch: 8 [43000/60000 (72%)]\tLoss: 0.014458\n",
      "Train Epoch: 8 [44000/60000 (73%)]\tLoss: 0.007765\n",
      "Train Epoch: 8 [45000/60000 (75%)]\tLoss: 0.027244\n",
      "Train Epoch: 8 [46000/60000 (77%)]\tLoss: 0.027311\n",
      "Train Epoch: 8 [47000/60000 (78%)]\tLoss: 0.059324\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.007310\n",
      "Train Epoch: 8 [49000/60000 (82%)]\tLoss: 0.059469\n",
      "Train Epoch: 8 [50000/60000 (83%)]\tLoss: 0.008777\n",
      "Train Epoch: 8 [51000/60000 (85%)]\tLoss: 0.022599\n",
      "Train Epoch: 8 [52000/60000 (87%)]\tLoss: 0.002171\n",
      "Train Epoch: 8 [53000/60000 (88%)]\tLoss: 0.105646\n",
      "Train Epoch: 8 [54000/60000 (90%)]\tLoss: 0.027766\n",
      "Train Epoch: 8 [55000/60000 (92%)]\tLoss: 0.013054\n",
      "Train Epoch: 8 [56000/60000 (93%)]\tLoss: 0.009562\n",
      "Train Epoch: 8 [57000/60000 (95%)]\tLoss: 0.029324\n",
      "Train Epoch: 8 [58000/60000 (97%)]\tLoss: 0.001145\n",
      "Train Epoch: 8 [59000/60000 (98%)]\tLoss: 0.000177\n",
      "\n",
      "Test set: Average loss: 0.0479, Accuracy: 9870/10000 (99%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.008793\n",
      "Train Epoch: 9 [1000/60000 (2%)]\tLoss: 0.034726\n",
      "Train Epoch: 9 [2000/60000 (3%)]\tLoss: 0.029579\n",
      "Train Epoch: 9 [3000/60000 (5%)]\tLoss: 0.094873\n",
      "Train Epoch: 9 [4000/60000 (7%)]\tLoss: 0.031399\n",
      "Train Epoch: 9 [5000/60000 (8%)]\tLoss: 0.056232\n",
      "Train Epoch: 9 [6000/60000 (10%)]\tLoss: 0.049025\n",
      "Train Epoch: 9 [7000/60000 (12%)]\tLoss: 0.061598\n",
      "Train Epoch: 9 [8000/60000 (13%)]\tLoss: 0.001775\n",
      "Train Epoch: 9 [9000/60000 (15%)]\tLoss: 0.023540\n",
      "Train Epoch: 9 [10000/60000 (17%)]\tLoss: 0.001906\n",
      "Train Epoch: 9 [11000/60000 (18%)]\tLoss: 0.004787\n",
      "Train Epoch: 9 [12000/60000 (20%)]\tLoss: 0.003232\n",
      "Train Epoch: 9 [13000/60000 (22%)]\tLoss: 0.038546\n",
      "Train Epoch: 9 [14000/60000 (23%)]\tLoss: 0.011832\n",
      "Train Epoch: 9 [15000/60000 (25%)]\tLoss: 0.002347\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.019403\n",
      "Train Epoch: 9 [17000/60000 (28%)]\tLoss: 0.024420\n",
      "Train Epoch: 9 [18000/60000 (30%)]\tLoss: 0.022823\n",
      "Train Epoch: 9 [19000/60000 (32%)]\tLoss: 0.019561\n",
      "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 0.061049\n",
      "Train Epoch: 9 [21000/60000 (35%)]\tLoss: 0.009238\n",
      "Train Epoch: 9 [22000/60000 (37%)]\tLoss: 0.000310\n",
      "Train Epoch: 9 [23000/60000 (38%)]\tLoss: 0.007488\n",
      "Train Epoch: 9 [24000/60000 (40%)]\tLoss: 0.001548\n",
      "Train Epoch: 9 [25000/60000 (42%)]\tLoss: 0.006963\n",
      "Train Epoch: 9 [26000/60000 (43%)]\tLoss: 0.007434\n",
      "Train Epoch: 9 [27000/60000 (45%)]\tLoss: 0.024957\n",
      "Train Epoch: 9 [28000/60000 (47%)]\tLoss: 0.023369\n",
      "Train Epoch: 9 [29000/60000 (48%)]\tLoss: 0.098520\n",
      "Train Epoch: 9 [30000/60000 (50%)]\tLoss: 0.120261\n",
      "Train Epoch: 9 [31000/60000 (52%)]\tLoss: 0.023303\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.003583\n",
      "Train Epoch: 9 [33000/60000 (55%)]\tLoss: 0.001513\n",
      "Train Epoch: 9 [34000/60000 (57%)]\tLoss: 0.012349\n",
      "Train Epoch: 9 [35000/60000 (58%)]\tLoss: 0.022094\n",
      "Train Epoch: 9 [36000/60000 (60%)]\tLoss: 0.025326\n",
      "Train Epoch: 9 [37000/60000 (62%)]\tLoss: 0.100708\n",
      "Train Epoch: 9 [38000/60000 (63%)]\tLoss: 0.051886\n",
      "Train Epoch: 9 [39000/60000 (65%)]\tLoss: 0.003740\n",
      "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 0.006873\n",
      "Train Epoch: 9 [41000/60000 (68%)]\tLoss: 0.049300\n",
      "Train Epoch: 9 [42000/60000 (70%)]\tLoss: 0.020467\n",
      "Train Epoch: 9 [43000/60000 (72%)]\tLoss: 0.139783\n",
      "Train Epoch: 9 [44000/60000 (73%)]\tLoss: 0.014975\n",
      "Train Epoch: 9 [45000/60000 (75%)]\tLoss: 0.058031\n",
      "Train Epoch: 9 [46000/60000 (77%)]\tLoss: 0.062676\n",
      "Train Epoch: 9 [47000/60000 (78%)]\tLoss: 0.044355\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.031599\n",
      "Train Epoch: 9 [49000/60000 (82%)]\tLoss: 0.032018\n",
      "Train Epoch: 9 [50000/60000 (83%)]\tLoss: 0.000346\n",
      "Train Epoch: 9 [51000/60000 (85%)]\tLoss: 0.000195\n",
      "Train Epoch: 9 [52000/60000 (87%)]\tLoss: 0.001875\n",
      "Train Epoch: 9 [53000/60000 (88%)]\tLoss: 0.103590\n",
      "Train Epoch: 9 [54000/60000 (90%)]\tLoss: 0.080866\n",
      "Train Epoch: 9 [55000/60000 (92%)]\tLoss: 0.010958\n",
      "Train Epoch: 9 [56000/60000 (93%)]\tLoss: 0.038818\n",
      "Train Epoch: 9 [57000/60000 (95%)]\tLoss: 0.025248\n",
      "Train Epoch: 9 [58000/60000 (97%)]\tLoss: 0.008564\n",
      "Train Epoch: 9 [59000/60000 (98%)]\tLoss: 0.000055\n",
      "\n",
      "Test set: Average loss: 0.0630, Accuracy: 9844/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 10):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
